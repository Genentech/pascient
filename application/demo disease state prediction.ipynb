{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../\") #load the cellm outside of notebooks\n",
    "sys.path.insert(0, \"../reproduce/\") #load the rep_utils outside of notebooks\n",
    "\n",
    "from rep_utils import SampleCellsDataModuleCustom, CellClassifyModel\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tiledb_base_path = '/projects/global/gred/resbioai/CeLLM/tiledb'\n",
    "\n",
    "CELLURI = \"scimilarity_human_10x_cell_metadata\"\n",
    "GENEURI = \"scimilarity_human_10x_gene_metadata\"\n",
    "COUNTSURI = \"scimilarity_human_10x_counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_path: please change the string with your model path\n",
    "attention: Attention design in the trained model\n",
    "batch_size: number of samples per batch used for testing\n",
    "sample_size: number of cells in each sample used for testing\n",
    "classify_mode: Type of classification (binary or multilabel)\n",
    "resample: wheter resampling cells from the same sample.\n",
    "'''\n",
    "\n",
    "model_path = '' \n",
    "attention = 'nonlinear_attn'\n",
    "batch_size = 1\n",
    "sample_size = 1500\n",
    "classify_mode = 'multilabel'\n",
    "resample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_model = CellClassifyModel.load_from_checkpoint(model_path, num_genes=28231, masking_strategy=None, attn = attention, classify_mode = classify_mode, ) \n",
    "\n",
    "class_model.eval()\n",
    "\n",
    "scd = SampleCellsDataModuleCustom(batch_size = batch_size, sample_size=sample_size, classify_mode =classify_mode, resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an option str.\n",
    "f1score_list = []\n",
    "finalauroc_list = []\n",
    "full_results = False # wether to predict for all seeds or not.\n",
    "\n",
    "if full_results == False:\n",
    "    with torch.no_grad():\n",
    "        seed = 0\n",
    "        seed_everything(seed, workers=True)\n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        auroc_list = []\n",
    "        for i in scd.test_dataloader():\n",
    "            output_annot = class_model.obtain_annotation(i, '0')\n",
    "            pred_label += list(output_annot[0].cpu().numpy())\n",
    "            true_label += list(i.disease_label.cpu().numpy())\n",
    "        print(set(true_label))\n",
    "        print(set(pred_label))\n",
    "        print(classification_report(true_label, pred_label, digits=4))\n",
    "        f1score_list.append(classification_report(true_label, pred_label, digits=4, output_dict=True)['weighted avg']['f1-score'])\n",
    "        print(f1score_list)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for seed in range(0,10):\n",
    "            seed_everything(seed, workers=True)\n",
    "            \n",
    "            true_label = []\n",
    "            pred_label = []\n",
    "            auroc_list = []\n",
    "            for i in scd.test_dataloader():\n",
    "                output_annot = class_model.obtain_annotation(i, '0')\n",
    "                pred_label += list(output_annot[0].cpu().numpy())\n",
    "                true_label += list(i.disease_label.cpu().numpy())\n",
    "            print(set(true_label))\n",
    "            print(set(pred_label))\n",
    "            print(classification_report(true_label, pred_label, digits=4))\n",
    "            print(classification_report(true_label, pred_label, digits=4, output_dict=True)['weighted avg']['f1-score'])\n",
    "            f1score_list.append(classification_report(true_label, pred_label, digits=4, output_dict=True)['weighted avg']['f1-score'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the weighted f1 score\n",
    "print(f1score_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
